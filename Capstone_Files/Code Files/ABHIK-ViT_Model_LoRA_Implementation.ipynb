{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ffbb41f757aa469b8e7e1deb988a3cd6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_91dd6e3ef7ba410d9eca9e2913e382d0",
              "IPY_MODEL_e032791e546d4fb7981fd94c3685f2bc",
              "IPY_MODEL_22facb5cd1e54cfcab5d666e3bbbb2fc"
            ],
            "layout": "IPY_MODEL_b3dd5322e70f442cb7c762508d4706c6"
          }
        },
        "91dd6e3ef7ba410d9eca9e2913e382d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3391995a56744cef98ead9a97e2697b8",
            "placeholder": "​",
            "style": "IPY_MODEL_278b9c4a807c4df3a2b8409604c11434",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "e032791e546d4fb7981fd94c3685f2bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_460785ee92b748db8f088a27bda20359",
            "max": 160,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c4e7acf0eea34cf990a76484eaa768f8",
            "value": 160
          }
        },
        "22facb5cd1e54cfcab5d666e3bbbb2fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36d1ce9d73cb4946953265c178c9c65f",
            "placeholder": "​",
            "style": "IPY_MODEL_bb9023c106554c20a48e9884e76115a5",
            "value": " 160/160 [00:00&lt;00:00, 11.9kB/s]"
          }
        },
        "b3dd5322e70f442cb7c762508d4706c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3391995a56744cef98ead9a97e2697b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "278b9c4a807c4df3a2b8409604c11434": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "460785ee92b748db8f088a27bda20359": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4e7acf0eea34cf990a76484eaa768f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "36d1ce9d73cb4946953265c178c9c65f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb9023c106554c20a48e9884e76115a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e922918023b2473f956fb169db7b10a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c44dc3d028b84805981edce10b22ed4b",
              "IPY_MODEL_533cf086c1554cbaaf0f3a9aaef71225",
              "IPY_MODEL_db0114ab34644429b6350632f68dd66e"
            ],
            "layout": "IPY_MODEL_707425a016b34b1eb47d6908fb9e5bb8"
          }
        },
        "c44dc3d028b84805981edce10b22ed4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b008a9480ddc4faeafe7c4e4e4da1b20",
            "placeholder": "​",
            "style": "IPY_MODEL_7df14eb92c9d413fb13b31bf4b1cb8b1",
            "value": "config.json: 100%"
          }
        },
        "533cf086c1554cbaaf0f3a9aaef71225": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ccaeaabef76c42b786ff79516313c0f2",
            "max": 502,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_347e0b4d64594f10b75c5a11d1d71e72",
            "value": 502
          }
        },
        "db0114ab34644429b6350632f68dd66e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42211f40ef434fd68f9e94363570d219",
            "placeholder": "​",
            "style": "IPY_MODEL_57aa6f48eae3484aad11c318bb7d6b67",
            "value": " 502/502 [00:00&lt;00:00, 40.9kB/s]"
          }
        },
        "707425a016b34b1eb47d6908fb9e5bb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b008a9480ddc4faeafe7c4e4e4da1b20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7df14eb92c9d413fb13b31bf4b1cb8b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ccaeaabef76c42b786ff79516313c0f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "347e0b4d64594f10b75c5a11d1d71e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42211f40ef434fd68f9e94363570d219": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57aa6f48eae3484aad11c318bb7d6b67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "666ec372e6a54efc8dd52a38308074f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bc1b76d469c742f7a1db5b2eaa8e9a5e",
              "IPY_MODEL_7cf83dafbb4c4d1289158095fd5e6d0b",
              "IPY_MODEL_80dcb783b6324e5c98c0ad186802bd3f"
            ],
            "layout": "IPY_MODEL_8a21b6acfba44bf4a9d0c9af19346a7a"
          }
        },
        "bc1b76d469c742f7a1db5b2eaa8e9a5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2d83dbc62a974d89ab21439878f33299",
            "placeholder": "​",
            "style": "IPY_MODEL_512eb2f6983b446b94eb5321526f0030",
            "value": "model.safetensors: 100%"
          }
        },
        "7cf83dafbb4c4d1289158095fd5e6d0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_845339d71e87488bb34d3d065a718477",
            "max": 345579424,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_eb193d9f39254f7183e94ee605b656c4",
            "value": 345579424
          }
        },
        "80dcb783b6324e5c98c0ad186802bd3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_709b1122e79d4c05a9d82d1febf97286",
            "placeholder": "​",
            "style": "IPY_MODEL_657b95b8aa63481b9ee23895218f4d45",
            "value": " 346M/346M [00:00&lt;00:00, 560MB/s]"
          }
        },
        "8a21b6acfba44bf4a9d0c9af19346a7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d83dbc62a974d89ab21439878f33299": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "512eb2f6983b446b94eb5321526f0030": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "845339d71e87488bb34d3d065a718477": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb193d9f39254f7183e94ee605b656c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "709b1122e79d4c05a9d82d1febf97286": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "657b95b8aa63481b9ee23895218f4d45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uLW21aUcB6TQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "316ed8ec-886c-427b-e60d-1ae8a1725707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from PIL import Image\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
        "model_checkpoint = \"google/vit-base-patch16-224-in21k\""
      ],
      "metadata": {
        "id": "2UvfGkGe4xno"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "ffbb41f757aa469b8e7e1deb988a3cd6",
            "91dd6e3ef7ba410d9eca9e2913e382d0",
            "e032791e546d4fb7981fd94c3685f2bc",
            "22facb5cd1e54cfcab5d666e3bbbb2fc",
            "b3dd5322e70f442cb7c762508d4706c6",
            "3391995a56744cef98ead9a97e2697b8",
            "278b9c4a807c4df3a2b8409604c11434",
            "460785ee92b748db8f088a27bda20359",
            "c4e7acf0eea34cf990a76484eaa768f8",
            "36d1ce9d73cb4946953265c178c9c65f",
            "bb9023c106554c20a48e9884e76115a5",
            "e922918023b2473f956fb169db7b10a7",
            "c44dc3d028b84805981edce10b22ed4b",
            "533cf086c1554cbaaf0f3a9aaef71225",
            "db0114ab34644429b6350632f68dd66e",
            "707425a016b34b1eb47d6908fb9e5bb8",
            "b008a9480ddc4faeafe7c4e4e4da1b20",
            "7df14eb92c9d413fb13b31bf4b1cb8b1",
            "ccaeaabef76c42b786ff79516313c0f2",
            "347e0b4d64594f10b75c5a11d1d71e72",
            "42211f40ef434fd68f9e94363570d219",
            "57aa6f48eae3484aad11c318bb7d6b67"
          ]
        },
        "id": "vXuOUsf24jvI",
        "outputId": "aa8e6c7a-6982-43ee-8da9-a4449bc63be6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/160 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ffbb41f757aa469b8e7e1deb988a3cd6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/502 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e922918023b2473f956fb169db7b10a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import (\n",
        "    CenterCrop,\n",
        "    Compose,\n",
        "    Normalize,\n",
        "    RandomHorizontalFlip,\n",
        "    RandomResizedCrop,\n",
        "    Resize,\n",
        "    ToTensor,\n",
        ")\n",
        "\n",
        "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
        "train_transforms = Compose(\n",
        "    [\n",
        "        RandomResizedCrop(image_processor.size[\"height\"]),\n",
        "        RandomHorizontalFlip(),\n",
        "        ToTensor(),\n",
        "        normalize,\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_transforms = Compose(\n",
        "    [\n",
        "        Resize(image_processor.size[\"height\"]),\n",
        "        CenterCrop(image_processor.size[\"height\"]),\n",
        "        ToTensor(),\n",
        "        normalize,\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "0rwFlLwK4OYZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Custom Dataset\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, root_dir, sub_folder, transform=None, data_type='original'):\n",
        "        self.root_dir = root_dir\n",
        "        self.sub_folder = sub_folder\n",
        "        self.transform = transform\n",
        "        self.data_type = data_type\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        if self.data_type == 'original':\n",
        "            self.load_original_data()\n",
        "        elif self.data_type == 'augmentation':\n",
        "            self.load_augmented_data()\n",
        "\n",
        "    def load_original_data(self):\n",
        "        label_file = os.path.join(self.root_dir, 'shortaxis_binary v2.xlsx')\n",
        "        for video_folder in os.listdir(os.path.join(self.root_dir, self.sub_folder)):\n",
        "            video_path = os.path.join(self.root_dir, self.sub_folder, video_folder)\n",
        "            if os.path.isdir(video_path):\n",
        "                try:\n",
        "                    labels_df = pd.read_excel(label_file, sheet_name=video_folder)\n",
        "                except ValueError:\n",
        "                    continue\n",
        "\n",
        "                for img_filename in os.listdir(video_path):\n",
        "                    if img_filename.endswith(\".jpg\"):\n",
        "                        img_path = os.path.join(video_path, img_filename)\n",
        "                        frame_idx = int(os.path.splitext(img_filename)[0].split('_')[-1])\n",
        "                        labels = labels_df.loc[frame_idx, ['BAD QUALITY', 'CORD', 'FLUID']].values.astype('float32')\n",
        "                        self.image_paths.append(img_path)\n",
        "                        self.labels.append(labels)\n",
        "\n",
        "    def load_augmented_data(self):\n",
        "        for video_folder in os.listdir(os.path.join(self.root_dir, self.sub_folder)):\n",
        "            video_path = os.path.join(self.root_dir, self.sub_folder, video_folder)\n",
        "            if os.path.isdir(video_path):\n",
        "                label_file = os.path.join(self.root_dir, 'Label', f'{video_folder}.xlsx')\n",
        "                labels_df = pd.read_excel(label_file)\n",
        "\n",
        "                for img_filename in os.listdir(video_path):\n",
        "                    if img_filename.endswith(\".jpg\"):\n",
        "                        img_path = os.path.join(video_path, img_filename)\n",
        "                        labels = labels_df.loc[labels_df['FILENAME'] == img_filename,\n",
        "                                               ['BAD QUALITY', 'CORD', 'FLUID']].values.astype('float32')\n",
        "                        self.image_paths.append(img_path)\n",
        "                        self.labels.append(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        labels = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, labels\n",
        "\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Subset\n",
        "\n",
        "def initialize_data(root_dir, sub_folder, data_type='original'):\n",
        "    # Define separate transforms for training and validation\n",
        "    # train_transform = transforms.Compose([\n",
        "    #     transforms.Resize((224, 224)),\n",
        "    #     transforms.RandomHorizontalFlip(),\n",
        "    #     transforms.ToTensor(),\n",
        "    # ])\n",
        "\n",
        "    # val_transform = transforms.Compose([\n",
        "    #     transforms.Resize((224, 224)),\n",
        "    #     transforms.ToTensor(),\n",
        "    # ])\n",
        "\n",
        "    # Load the full dataset\n",
        "    full_dataset = CustomDataset(root_dir, sub_folder=sub_folder, data_type=data_type)\n",
        "\n",
        "    # Use sklearn's train_test_split to split into train and temp (validation + test)\n",
        "    indices = list(range(len(full_dataset)))\n",
        "    train_indices, temp_indices = train_test_split(indices, test_size=0.3, random_state=42)\n",
        "\n",
        "    # Further split temp into validation and test sets\n",
        "    val_indices, test_indices = train_test_split(temp_indices, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Create subsets for train, val, and test\n",
        "    train_dataset = Subset(full_dataset, train_indices)\n",
        "    val_dataset = Subset(full_dataset, val_indices)\n",
        "    test_dataset = Subset(full_dataset, test_indices)\n",
        "\n",
        "    # Apply the appropriate transforms to each subset\n",
        "    train_dataset.dataset.transform = train_transforms\n",
        "    val_dataset.dataset.transform = val_transforms\n",
        "    test_dataset.dataset.transform = val_transforms  # Optionally reuse val_transform for test set\n",
        "\n",
        "    print(f'Train Size: {len(train_dataset)}, Val Size: {len(val_dataset)}, Test Size: {len(test_dataset)}')\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset"
      ],
      "metadata": {
        "id": "FLMZSg3UCKR7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Data\n",
        "root_dir = '/content/drive/MyDrive'    # CHANGE BASED ON FOLDER LOCATION\n",
        "sub_folder = 'short axis frames'\n",
        "num_classes=3\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "org_train_data, org_val_data, org_test_data = initialize_data(root_dir,\n",
        "                                                                    sub_folder,\n",
        "                                                                    data_type='original')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WOc89cTzC26B",
        "outputId": "d6c6c7c9-2962-4ffa-bf76-f2e8f39840e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Size: 2373, Val Size: 508, Test Size: 509\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAYJB-HqkF2d",
        "outputId": "51cf1c78-d4ba-423f-f395-050df6bd1c21"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming your labels are: ['BAD QUALITY', 'CORD', 'FLUID']\n",
        "labels = ['BAD QUALITY', 'CORD', 'FLUID']\n",
        "\n",
        "# Creating label-to-ID and ID-to-label mappings\n",
        "label2id = {label: idx for idx, label in enumerate(labels)}\n",
        "id2label = {idx: label for idx, label in enumerate(labels)}\n",
        "\n",
        "# Checking the value for ID 2\n",
        "print(id2label[2])  # Output: 'FLUID'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMhevGPsLTGN",
        "outputId": "53d0c4a1-7b11-4a26-be8f-ed20e0d07fdf"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FLUID\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate evaluate datasets peft -q"
      ],
      "metadata": {
        "id": "RgvohZ61J9iy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c03a8b2b-0d84-4e0c-d177-e62df750372a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/84.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/472.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m472.7/472.7 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/320.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.7/320.7 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import accelerate\n",
        "import peft\n",
        "\n",
        "print(f\"Transformers version: {transformers.__version__}\")\n",
        "print(f\"Accelerate version: {accelerate.__version__}\")\n",
        "print(f\"PEFT version: {peft.__version__}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4ouLv9DKaVu",
        "outputId": "2cad7e0e-dd3f-4d35-a0f7-9ca3cb411296"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transformers version: 4.44.2\n",
            "Accelerate version: 0.34.2\n",
            "PEFT version: 0.13.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint = \"google/vit-base-patch16-224-in21k\""
      ],
      "metadata": {
        "id": "AamadpcuEqIY"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoImageProcessor\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "jlF5mENKMcAT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b86d46ca-6947-4fb7-8312-304eaf0dcb39"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Fast image processor class <class 'transformers.models.vit.image_processing_vit_fast.ViTImageProcessorFast'> is available for this model. Using slow image processor class. To use the fast image processor class set `use_fast=True`.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param:.2f}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "XEx2fDVU6yWv"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForImageClassification.from_pretrained(\n",
        "    model_checkpoint,\n",
        "    label2id=label2id,\n",
        "    id2label=id2label,\n",
        "    ignore_mismatched_sizes=True,  # provide this in case you're planning to fine-tune an already fine-tuned checkpoint\n",
        ")"
      ],
      "metadata": {
        "id": "BJzw3lHBRAa4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104,
          "referenced_widgets": [
            "666ec372e6a54efc8dd52a38308074f9",
            "bc1b76d469c742f7a1db5b2eaa8e9a5e",
            "7cf83dafbb4c4d1289158095fd5e6d0b",
            "80dcb783b6324e5c98c0ad186802bd3f",
            "8a21b6acfba44bf4a9d0c9af19346a7a",
            "2d83dbc62a974d89ab21439878f33299",
            "512eb2f6983b446b94eb5321526f0030",
            "845339d71e87488bb34d3d065a718477",
            "eb193d9f39254f7183e94ee605b656c4",
            "709b1122e79d4c05a9d82d1febf97286",
            "657b95b8aa63481b9ee23895218f4d45"
          ]
        },
        "outputId": "10b17b16-f5bc-45fc-dd13-4459b1e80006"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "666ec372e6a54efc8dd52a38308074f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gbCXNOw64oP",
        "outputId": "8bd334f0-d72e-429e-deed-d1d03172daab"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 85800963 || all params: 85800963 || trainable%: 100.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for name, module in model.named_modules():\n",
        "    print(name, \":\", module)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cy5xPJTM69OP",
        "outputId": "e7b26f04-9ef4-49dd-8ff5-aea5332c7d0a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " : ViTForImageClassification(\n",
            "  (vit): ViTModel(\n",
            "    (embeddings): ViTEmbeddings(\n",
            "      (patch_embeddings): ViTPatchEmbeddings(\n",
            "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "      )\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (encoder): ViTEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x ViTLayer(\n",
            "          (attention): ViTSdpaAttention(\n",
            "            (attention): ViTSdpaSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (output): ViTSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): ViTIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): ViTOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
            ")\n",
            "vit : ViTModel(\n",
            "  (embeddings): ViTEmbeddings(\n",
            "    (patch_embeddings): ViTPatchEmbeddings(\n",
            "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "    )\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (encoder): ViTEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0-11): 12 x ViTLayer(\n",
            "        (attention): ViTSdpaAttention(\n",
            "          (attention): ViTSdpaSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (output): ViTSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): ViTIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (intermediate_act_fn): GELUActivation()\n",
            "        )\n",
            "        (output): ViTOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.embeddings : ViTEmbeddings(\n",
            "  (patch_embeddings): ViTPatchEmbeddings(\n",
            "    (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "  )\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.embeddings.patch_embeddings : ViTPatchEmbeddings(\n",
            "  (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            ")\n",
            "vit.embeddings.patch_embeddings.projection : Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "vit.embeddings.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder : ViTEncoder(\n",
            "  (layer): ModuleList(\n",
            "    (0-11): 12 x ViTLayer(\n",
            "      (attention): ViTSdpaAttention(\n",
            "        (attention): ViTSdpaSelfAttention(\n",
            "          (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "        (output): ViTSelfOutput(\n",
            "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (intermediate): ViTIntermediate(\n",
            "        (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        (intermediate_act_fn): GELUActivation()\n",
            "      )\n",
            "      (output): ViTOutput(\n",
            "        (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "      (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer : ModuleList(\n",
            "  (0-11): 12 x ViTLayer(\n",
            "    (attention): ViTSdpaAttention(\n",
            "      (attention): ViTSdpaSelfAttention(\n",
            "        (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "      (output): ViTSelfOutput(\n",
            "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "        (dropout): Dropout(p=0.0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (intermediate): ViTIntermediate(\n",
            "      (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "      (intermediate_act_fn): GELUActivation()\n",
            "    )\n",
            "    (output): ViTOutput(\n",
            "      (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.0 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.0.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.0.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.0.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.0.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.0.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.0.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.0.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.0.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.0.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.0.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.0.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.0.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.0.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.0.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.0.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.0.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.0.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.1 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.1.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.1.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.1.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.1.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.1.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.1.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.1.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.1.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.1.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.1.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.1.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.1.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.1.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.1.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.1.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.1.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.1.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.2 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.2.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.2.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.2.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.2.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.2.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.2.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.2.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.2.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.2.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.2.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.2.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.2.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.2.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.2.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.2.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.2.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.2.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.3 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.3.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.3.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.3.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.3.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.3.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.3.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.3.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.3.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.3.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.3.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.3.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.3.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.3.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.3.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.3.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.3.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.3.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.4 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.4.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.4.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.4.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.4.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.4.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.4.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.4.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.4.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.4.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.4.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.4.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.4.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.4.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.4.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.4.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.4.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.4.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.5 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.5.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.5.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.5.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.5.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.5.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.5.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.5.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.5.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.5.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.5.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.5.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.5.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.5.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.5.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.5.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.5.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.5.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.6 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.6.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.6.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.6.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.6.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.6.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.6.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.6.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.6.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.6.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.6.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.6.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.6.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.6.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.6.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.6.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.6.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.6.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.7 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.7.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.7.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.7.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.7.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.7.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.7.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.7.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.7.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.7.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.7.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.7.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.7.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.7.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.7.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.7.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.7.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.7.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.8 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.8.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.8.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.8.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.8.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.8.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.8.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.8.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.8.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.8.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.8.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.8.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.8.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.8.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.8.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.8.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.8.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.8.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.9 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.9.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.9.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.9.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.9.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.9.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.9.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.9.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.9.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.9.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.9.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.9.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.9.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.9.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.9.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.9.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.9.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.9.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.10 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.10.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.10.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.10.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.10.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.10.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.10.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.10.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.10.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.10.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.10.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.10.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.10.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.10.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.10.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.10.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.10.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.10.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.11 : ViTLayer(\n",
            "  (attention): ViTSdpaAttention(\n",
            "    (attention): ViTSdpaSelfAttention(\n",
            "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (output): ViTSelfOutput(\n",
            "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "  )\n",
            "  (intermediate): ViTIntermediate(\n",
            "    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "    (intermediate_act_fn): GELUActivation()\n",
            "  )\n",
            "  (output): ViTOutput(\n",
            "    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            ")\n",
            "vit.encoder.layer.11.attention : ViTSdpaAttention(\n",
            "  (attention): ViTSdpaSelfAttention(\n",
            "    (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            "  (output): ViTSelfOutput(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (dropout): Dropout(p=0.0, inplace=False)\n",
            "  )\n",
            ")\n",
            "vit.encoder.layer.11.attention.attention : ViTSdpaSelfAttention(\n",
            "  (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.11.attention.attention.query : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.11.attention.attention.key : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.11.attention.attention.value : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.11.attention.attention.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.11.attention.output : ViTSelfOutput(\n",
            "  (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.11.attention.output.dense : Linear(in_features=768, out_features=768, bias=True)\n",
            "vit.encoder.layer.11.attention.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.11.intermediate : ViTIntermediate(\n",
            "  (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "  (intermediate_act_fn): GELUActivation()\n",
            ")\n",
            "vit.encoder.layer.11.intermediate.dense : Linear(in_features=768, out_features=3072, bias=True)\n",
            "vit.encoder.layer.11.intermediate.intermediate_act_fn : GELUActivation()\n",
            "vit.encoder.layer.11.output : ViTOutput(\n",
            "  (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "  (dropout): Dropout(p=0.0, inplace=False)\n",
            ")\n",
            "vit.encoder.layer.11.output.dense : Linear(in_features=3072, out_features=768, bias=True)\n",
            "vit.encoder.layer.11.output.dropout : Dropout(p=0.0, inplace=False)\n",
            "vit.encoder.layer.11.layernorm_before : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.encoder.layer.11.layernorm_after : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "vit.layernorm : LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "classifier : Linear(in_features=768, out_features=3, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    modules_to_save=[\"classifier\"],\n",
        ")\n",
        "lora_model = get_peft_model(model, config)\n",
        "print_trainable_parameters(lora_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmbkwaU_6_3p",
        "outputId": "83d0a23d-9f8d-4686-b5c2-d8ea39059406"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 2656515 || all params: 88457478 || trainable%: 3.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "eKaLy97j3324"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# def compute_metrics(eval_pred):\n",
        "#     predictions, references = eval_pred\n",
        "\n",
        "#     # Convert multi-label references (e.g., [1, 0, 0]) to class indices (e.g., 0)\n",
        "#     references = np.argmax(references, axis=1)\n",
        "\n",
        "#     # If predictions are probabilities, convert them to class indices\n",
        "#     if predictions.ndim > 1:\n",
        "#         predictions = np.argmax(predictions, axis=1)\n",
        "\n",
        "#     # Compute accuracy and F1-score\n",
        "#     accuracy = accuracy_score(references, predictions)\n",
        "#     f1 = f1_score(references, predictions, average='weighted')\n",
        "\n",
        "#     return {\n",
        "#         \"accuracy\": accuracy,\n",
        "#         \"f1\": f1,\n",
        "#     }\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, hamming_loss, jaccard_score\n",
        "\n",
        "def compute_metrics(eval_pred, threshold=0.5):\n",
        "    predictions, references = eval_pred\n",
        "\n",
        "    # Apply sigmoid if necessary\n",
        "    predictions = 1 / (1 + np.exp(-predictions)) if predictions.ndim > 1 else predictions\n",
        "\n",
        "    # Binarize predictions\n",
        "    predictions = (predictions >= threshold).astype(int)\n",
        "    references = (references >= 0.5).astype(int)  # Assuming references are probabilities\n",
        "\n",
        "    # Compute metrics\n",
        "    accuracy = accuracy_score(references, predictions)\n",
        "    f1_macro = f1_score(references, predictions, average='macro')\n",
        "    f1_micro = f1_score(references, predictions, average='micro')\n",
        "    f1_samples = f1_score(references, predictions, average='samples')\n",
        "    hamming = hamming_loss(references, predictions)\n",
        "    jaccard = jaccard_score(references, predictions, average='macro')\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"f1_macro\": f1_macro,\n",
        "        \"f1_micro\": f1_micro,\n",
        "        \"f1_samples\": f1_samples,\n",
        "        \"hamming_loss\": hamming,\n",
        "        \"jaccard\": jaccard,\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "0utG5J18330c"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "\n",
        "\n",
        "# def collate_fn(examples):\n",
        "#     # Unpack the tuples into separate lists\n",
        "#     pixel_values = torch.stack([example[0] for example in examples])\n",
        "#     labels = torch.tensor(np.array([example[1] for example in examples]))\n",
        "#     return {\"pixel_values\": pixel_values, \"labels\": labels}\n",
        "\n",
        "import torch\n",
        "\n",
        "def collate_fn(examples):\n",
        "    # Unpack the tuples into separate lists\n",
        "    pixel_values = torch.stack([example[0] for example in examples])\n",
        "\n",
        "    # Directly convert labels to a tensor with specified dtype\n",
        "    labels = torch.stack([torch.tensor(example[1], dtype=torch.float32) for example in examples])\n",
        "\n",
        "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\""
      ],
      "metadata": {
        "id": "SfewVe4X9pM7"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import logging\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Enable logging and tqdm\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "from transformers.utils import logging as hf_logging\n",
        "hf_logging.set_verbosity_info()\n",
        "\n",
        "# Ensure the model is loaded on the GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "model_name = model_checkpoint.split(\"/\")[-1]\n",
        "batch_size = 128\n",
        "\n",
        "args = TrainingArguments(\n",
        "    f\"{model_name}-finetuned-lora-usg\",\n",
        "    remove_unused_columns=False,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-3,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    gradient_accumulation_steps=4,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    num_train_epochs=10,\n",
        "    logging_steps=5,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    push_to_hub=False,\n",
        "    label_names=[\"labels\"],\n",
        "    disable_tqdm=False,  # Ensure tqdm is enabled\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    lora_model,\n",
        "    args,\n",
        "    train_dataset=org_train_data,\n",
        "    eval_dataset=org_val_data,\n",
        "    tokenizer=image_processor,\n",
        "    compute_metrics=compute_metrics,\n",
        "    data_collator=collate_fn,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kidc2qh_7TQL",
        "outputId": "adb3fbe0-135e-4438-f665-185973e414c5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "PyTorch: setting up devices\n",
            "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "Using auto half precision backend\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start training\n",
        "train_results = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9LOw4EbU7X3z",
        "outputId": "bfcf137c-de70-45ff-a754-1c94b26f608d"
      },
      "execution_count": 24,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "***** Running training *****\n",
            "  Num examples = 2,373\n",
            "  Num Epochs = 10\n",
            "  Instantaneous batch size per device = 128\n",
            "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
            "  Gradient Accumulation steps = 4\n",
            "  Total optimization steps = 40\n",
            "  Number of trainable parameters = 2,656,515\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [40/40 36:33, Epoch 8/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Micro</th>\n",
              "      <th>F1 Samples</th>\n",
              "      <th>Hamming Loss</th>\n",
              "      <th>Jaccard</th>\n",
              "      <th>Runtime</th>\n",
              "      <th>Samples Per Second</th>\n",
              "      <th>Steps Per Second</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.516533</td>\n",
              "      <td>0.511811</td>\n",
              "      <td>0.275145</td>\n",
              "      <td>0.666045</td>\n",
              "      <td>0.639108</td>\n",
              "      <td>0.234908</td>\n",
              "      <td>0.234252</td>\n",
              "      <td>440.368900</td>\n",
              "      <td>1.154000</td>\n",
              "      <td>0.009000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.560000</td>\n",
              "      <td>0.453428</td>\n",
              "      <td>0.527559</td>\n",
              "      <td>0.329330</td>\n",
              "      <td>0.683521</td>\n",
              "      <td>0.654856</td>\n",
              "      <td>0.221785</td>\n",
              "      <td>0.267130</td>\n",
              "      <td>4.010000</td>\n",
              "      <td>126.682000</td>\n",
              "      <td>0.997000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.453200</td>\n",
              "      <td>0.328997</td>\n",
              "      <td>0.649606</td>\n",
              "      <td>0.687252</td>\n",
              "      <td>0.787766</td>\n",
              "      <td>0.733596</td>\n",
              "      <td>0.150262</td>\n",
              "      <td>0.550591</td>\n",
              "      <td>3.985000</td>\n",
              "      <td>127.479000</td>\n",
              "      <td>1.004000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.252300</td>\n",
              "      <td>0.170475</td>\n",
              "      <td>0.885827</td>\n",
              "      <td>0.912638</td>\n",
              "      <td>0.935829</td>\n",
              "      <td>0.862205</td>\n",
              "      <td>0.047244</td>\n",
              "      <td>0.842170</td>\n",
              "      <td>4.146500</td>\n",
              "      <td>122.514000</td>\n",
              "      <td>0.965000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.172400</td>\n",
              "      <td>0.131322</td>\n",
              "      <td>0.917323</td>\n",
              "      <td>0.929893</td>\n",
              "      <td>0.948109</td>\n",
              "      <td>0.870079</td>\n",
              "      <td>0.038714</td>\n",
              "      <td>0.870331</td>\n",
              "      <td>3.961000</td>\n",
              "      <td>128.250000</td>\n",
              "      <td>1.010000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.120700</td>\n",
              "      <td>0.109710</td>\n",
              "      <td>0.937008</td>\n",
              "      <td>0.949364</td>\n",
              "      <td>0.959649</td>\n",
              "      <td>0.885827</td>\n",
              "      <td>0.030184</td>\n",
              "      <td>0.904107</td>\n",
              "      <td>4.049000</td>\n",
              "      <td>125.462000</td>\n",
              "      <td>0.988000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.074100</td>\n",
              "      <td>0.094520</td>\n",
              "      <td>0.948819</td>\n",
              "      <td>0.962718</td>\n",
              "      <td>0.969325</td>\n",
              "      <td>0.892388</td>\n",
              "      <td>0.022966</td>\n",
              "      <td>0.928403</td>\n",
              "      <td>4.500000</td>\n",
              "      <td>112.890000</td>\n",
              "      <td>0.889000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 508\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-4\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Image processor saved in vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-4/preprocessor_config.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 508\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-9\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Image processor saved in vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-9/preprocessor_config.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 508\n",
            "  Batch size = 128\n",
            "Saving model checkpoint to vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-14\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Image processor saved in vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-14/preprocessor_config.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 508\n",
            "  Batch size = 128\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "Saving model checkpoint to vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-19\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Image processor saved in vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-19/preprocessor_config.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 508\n",
            "  Batch size = 128\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "Saving model checkpoint to vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-23\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Image processor saved in vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-23/preprocessor_config.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 508\n",
            "  Batch size = 128\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "Saving model checkpoint to vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-28\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Image processor saved in vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-28/preprocessor_config.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 508\n",
            "  Batch size = 128\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "Saving model checkpoint to vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-33\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Image processor saved in vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-33/preprocessor_config.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 508\n",
            "  Batch size = 128\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "Saving model checkpoint to vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-38\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Image processor saved in vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-38/preprocessor_config.json\n",
            "Saving model checkpoint to vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-40\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Image processor saved in vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-40/preprocessor_config.json\n",
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 508\n",
            "  Batch size = 128\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "Saving model checkpoint to vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-40\n",
            "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
            "Image processor saved in vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-40/preprocessor_config.json\n",
            "\n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "Loading best model from vit-base-patch16-224-in21k-finetuned-lora-usg/checkpoint-40 (score: 0.9488188976377953).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(org_test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 352
        },
        "id": "EjvIG1kn7jzu",
        "outputId": "146cf510-26cf-44f6-f50f-6a5283b9547e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "***** Running Evaluation *****\n",
            "  Num examples = 509\n",
            "  Batch size = 128\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [4/4 03:41]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in samples with no true nor predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.10113943368196487,\n",
              " 'eval_accuracy': 0.9430255402750491,\n",
              " 'eval_f1_macro': 0.9499800598464132,\n",
              " 'eval_f1_micro': 0.9653333333333334,\n",
              " 'eval_f1_samples': 0.8795022920759659,\n",
              " 'eval_hamming_loss': 0.025540275049115914,\n",
              " 'eval_jaccard': 0.9055484153244552,\n",
              " 'eval_runtime': 450.4169,\n",
              " 'eval_samples_per_second': 1.13,\n",
              " 'eval_steps_per_second': 0.009,\n",
              " 'epoch': 8.421052631578947}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r  /content/vit-base-patch16-224-in21k-finetuned-lora-usg /content/drive/MyDrive/lora_weights_vit_base_patch_16/"
      ],
      "metadata": {
        "id": "Dz_X9nhnQiw6"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "f6XrGImEyhFi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28afd554-93bf-4df1-8021-b03af3b45b18"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Oct 22 18:06:56 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   30C    P0              50W / 400W |  13449MiB / 40960MiB |      0%      Default |\n",
            "|                                         |                      |             Disabled |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mx3fqkBn7l_3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
