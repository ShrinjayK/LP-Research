{"cells":[{"cell_type":"markdown","metadata":{"id":"YyHMSWSEik-u"},"source":["Completed Testing"]},{"cell_type":"markdown","metadata":{"id":"nMrdwLGdHnWv"},"source":["import os\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torchvision.models import resnet18, ResNet18_Weights\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from PIL import Image\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","from tqdm import tqdm\n","import numpy as np\n","from sklearn.metrics import classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","import sys\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9802,"status":"ok","timestamp":1731617218179,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"},"user_tz":300},"id":"HnM_DEZcFjN-","outputId":"f97556e9-c8aa-404c-b5e1-c8a2f30ba49d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["import os\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","from torchvision.models import resnet18, ResNet18_Weights\n","import torchvision.transforms as transforms\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from PIL import Image\n","from sklearn.metrics import accuracy_score, precision_score, f1_score, roc_auc_score, recall_score\n","from tqdm import tqdm\n","import numpy as np\n","from sklearn.metrics import classification_report, confusion_matrix\n","import matplotlib.pyplot as plt\n","import sys\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"i9rwBZn2feRA","executionInfo":{"status":"ok","timestamp":1731617218179,"user_tz":300,"elapsed":4,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"}}},"outputs":[],"source":["# Define Custom Dataset\n","class CustomDataset(Dataset):\n","    def __init__(self, root_dir, sub_folder, transform, data_type='original'):\n","        self.root_dir = root_dir\n","        self.sub_folder = sub_folder\n","        self.transform = transform\n","        self.data_type = data_type\n","        self.image_paths = []\n","        self.labels = []\n","\n","        if self.data_type == 'original':\n","            self.load_original_data()\n","        elif self.data_type == 'augmentation':\n","            self.load_augmented_data()\n","\n","    # Iterate through the video folders\n","    def load_original_data(self):\n","        label_file = os.path.join(root_dir, 'old + new image frames.xlsx')\n","        numVideos = 0\n","        for video_folder in os.listdir(os.path.join(root_dir, sub_folder)):\n","            if os.path.isdir(os.path.join(root_dir, sub_folder, video_folder)):\n","                video_path = os.path.join(root_dir, sub_folder, video_folder)\n","                try:\n","                    labels_df = pd.read_excel(label_file, sheet_name=f'{video_folder}')\n","                    print(f\"Processing video folder: {video_folder}\")\n","                    numVideos += 1\n","                except ValueError:\n","                    # If the sheet does not exist, skip this folder and continue with the next\n","                    continue\n","\n","                # Iterate through image files and corresponding labels\n","                for img_filename in os.listdir(video_path):\n","                    if img_filename.endswith(\".jpg\"):\n","                        img_path = os.path.join(video_path, img_filename)\n","                        root, ext = os.path.splitext(img_filename)  # Split xxx_0.jpg into root and extension\n","                        frame_idx = int(root.split('_')[-1]) #splitting xxx_0 and storing 0 to frame_idx\n","                        labels = labels_df.loc[frame_idx, ['BAD QUALITY','CORD','FLUID']].values.astype('float32').squeeze()\n","\n","                        #print(img_path)\n","                        #print(labels)\n","\n","                        self.image_paths.append(img_path)\n","                        self.labels.append(labels)\n","        print(f\"Number of videos: {numVideos}\")\n","\n","    def load_augmented_data(self):\n","        for video_folder in os.listdir(os.path.join(root_dir, sub_folder)):\n","            if os.path.isdir(os.path.join(root_dir, sub_folder, video_folder)):\n","                video_path = os.path.join(root_dir, sub_folder, video_folder)\n","                label_file = os.path.join(root_dir, 'Label',f'{video_folder}.xlsx')\n","                labels_df = pd.read_excel(label_file)\n","\n","                # Iterate through image files and corresponding labels\n","                for img_filename in os.listdir(video_path):\n","                    if img_filename.endswith(\".jpg\"):\n","                        img_path = os.path.join(video_path, img_filename)\n","                        root, ext = os.path.splitext(img_filename)  # Split 0.jpg_xxxxx.jpg into root and extension\n","                        labels = labels_df.loc[labels_df['FILENAME']==img_filename, ['BAD QUALITY','CORD','FLUID']].values.astype('float32').squeeze()\n","\n","                        self.image_paths.append(img_path)\n","                        self.labels.append(labels)\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        image = Image.open(img_path).convert('RGB')\n","        labels = self.labels[idx]\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        return image, labels\n","\n","\n","# Initialize Dataset and Dataloader\n","def initialize_data(root_dir, sub_folder, data_type='original'):\n","    # Define image transformations\n","    transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor(),])\n","    train_dataset = CustomDataset(root_dir, sub_folder = sub_folder, transform=transform, data_type=data_type)\n","\n","    # Define the sizes for training, validation, and test sets\n","    total_size = len(train_dataset)\n","    train_size = int(0.70 * total_size)\n","    val_size = int(0.1 * total_size)\n","    test_size = total_size - train_size - val_size\n","\n","    # Use random_split to split the dataset\n","    # torch.cuda.manual_seed_all(0)  # Setting the random seed\n","    train_subset, val_subset, test_subset = random_split(train_dataset, [train_size, val_size, test_size])\n","\n","    # Create data loaders for each subset\n","    batch_size = 64\n","    train_loader = DataLoader(train_subset, batch_size=32, shuffle=True)\n","    val_loader = DataLoader(val_subset, batch_size=32, shuffle=False)\n","    test_loader = DataLoader(test_subset, batch_size=32, shuffle=False)\n","    print('Data Size: train:',len(train_loader.dataset),'val:',len(val_loader.dataset),'test:',len(test_loader.dataset))\n","\n","    return train_loader, val_loader, test_loader"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"DoO92OkMkjb7","executionInfo":{"status":"ok","timestamp":1731617218179,"user_tz":300,"elapsed":3,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"}}},"outputs":[],"source":["# Define Model\n","class CustomResNet(nn.Module):\n","    def __init__(self, num_classes):\n","        super(CustomResNet, self).__init__()\n","        self.resnet = resnet18(weights=ResNet18_Weights.IMAGENET1K_V1)\n","        num_features = self.resnet.fc.in_features\n","        self.resnet.fc = nn.Sequential(\n","            nn.Linear(num_features, num_classes),\n","            nn.Sigmoid()  # Sigmoid activation for multi-label classification\n","        )\n","\n","    def forward(self, x):\n","        return self.resnet(x)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"qEbhuhoIlAKE","executionInfo":{"status":"ok","timestamp":1731617218179,"user_tz":300,"elapsed":3,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"}}},"outputs":[],"source":["# Train Model\n","def train_model(train_loader, val_loader, model, criterion, optimizer, num_epochs):\n","    global num_classes\n","    global device\n","    train_losses = []\n","    train_accuracies = []\n","    val_losses = []\n","    val_accuracies = []\n","\n","    for epoch in range(num_epochs):\n","        model.train()\n","        # Initialize loss and accuracy variables for this epoch\n","        running_loss = 0.0\n","        correct_predictions = 0\n","\n","        # Initialize the progress bar\n","        train_progress_bar = tqdm(train_loader, total=len(train_loader), desc=f'Epoch {epoch + 1}', position=0,leave=True)\n","\n","        for images, labels in train_progress_bar:\n","            images, labels = images.to(device), labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(images)\n","            predicts = (outputs > 0.5).float()\n","            acc = (predicts == labels).sum().item() / (images.size(0) * num_classes)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss = loss.item()\n","            running_loss += train_loss * images.size(0)\n","            correct_predictions += acc * images.size(0)\n","\n","            # Update the progress bar with the loss and accuracy\n","            train_progress_bar.set_postfix({'Batch Loss': train_loss, 'Batch Accuracy': acc})\n","            torch.save(model, \"saved_model.pt\")\n","\n","        # Calculate average loss and accuracy for the epoch\n","        epoch_loss = running_loss / len(train_loader.dataset)\n","        epoch_accuracy = correct_predictions / len(train_loader.dataset)\n","        train_losses.append(epoch_loss)\n","        train_accuracies.append(epoch_accuracy)\n","        print(f'Epoch {epoch+1}/{num_epochs} - Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}')\n","\n","        # Save the state of the model\n","        torch.save(model.state_dict(), \"saved_model_state.pt\")\n","        torch.save(model, \"saved_model.pt\")\n","\n","        ######################### Validation loop\n","        print('Validation:')\n","        model.eval()\n","        val_running_loss = 0.0\n","        val_correct_predictions = 0\n","\n","        # Initialize the progress bar for validation\n","        val_progress_bar = tqdm(val_loader, total=len(val_loader), desc=f'Epoch {epoch + 1}', position=0,leave=True)\n","\n","        with torch.no_grad():\n","            for images, labels in val_progress_bar:\n","                images, labels = images.to(device), labels.to(device)\n","                outputs = model(images)\n","                predicts = (outputs > 0.5).float()\n","                acc = (predicts == labels).sum().item() / (images.size(0) * num_classes)\n","                loss = criterion(outputs, labels)\n","\n","                val_loss = loss.item()\n","                val_running_loss += val_loss * images.size(0)\n","                val_correct_predictions += acc * images.size(0)\n","\n","                # Update the progress bar with the loss and accuracy\n","                val_progress_bar.set_postfix({'Batch Loss': val_loss, 'Batch Accuracy': acc})\n","        # Calculate average loss and accuracy for the validation\n","        val_epoch_loss = val_running_loss / len(val_loader.dataset)\n","        val_epoch_accuracy = val_correct_predictions / len(val_loader.dataset)\n","        val_losses.append(val_epoch_loss)\n","        val_accuracies.append(val_epoch_accuracy)\n","        print(f'Validation Epoch {epoch+1}/{num_epochs} - Loss: {val_epoch_loss:.4f}, Accuracy: {val_epoch_accuracy:.4f}')\n","        print('------------------------------------------------------------------------------------------------')\n","\n","    return train_losses, train_accuracies, val_losses, val_accuracies\n","\n","def plot_loss_acc(train_losses, train_accuracies, val_losses, val_accuracies):\n","    # Plotting the training and validation loss\n","    plt.figure(figsize=(10, 5))\n","    plt.subplot(1, 2, 1)\n","    plt.plot(train_losses, label='Train Loss')\n","    plt.plot(val_losses, label='Validation Loss')\n","    plt.title('Epoch vs Loss')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Loss')\n","    plt.legend()\n","    # Plotting the training and validation accuracy\n","    plt.subplot(1, 2, 2)\n","    plt.plot(train_accuracies, label='Train Accuracy')\n","    plt.plot(val_accuracies, label='Validation Accuracy')\n","    plt.title('Epoch vs Accuracy')\n","    plt.xlabel('Epochs')\n","    plt.ylabel('Accuracy')\n","    plt.legend()\n","    plt.tight_layout()\n","    plt.grid()\n","    plt.show()"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"pwXm7_n692Wk","executionInfo":{"status":"ok","timestamp":1731617218179,"user_tz":300,"elapsed":3,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"}}},"outputs":[],"source":["# Load Model\n","def load_model(model_path):\n","    # Load the saved model\n","    model = torch.load(model_path)\n","    model.eval()  # Set the model to evaluation mode\n","    return model"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"tgV8syztlXPM","executionInfo":{"status":"ok","timestamp":1731617218179,"user_tz":300,"elapsed":3,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"}}},"outputs":[],"source":["# Test Model\n","def test_model(test_loader, model_path, criterion):\n","    global num_classes\n","    global device\n","    # Initialize variables to store predictions and true labels\n","\n","    # Load the model\n","    model = load_model(model_path)\n","\n","    all_preds = []\n","    all_labels = []\n","\n","    # Test loop\n","    model.eval()  # Set the model to evaluation mode\n","    test_running_loss = 0.0\n","    test_correct_predictions = 0\n","\n","    # Initialize the progress bar for testing\n","    test_progress_bar = tqdm(test_loader, total=len(test_loader), desc='Testing', position=0, leave=True)\n","\n","    misclassified_images = []\n","\n","    with torch.no_grad():\n","        image_index = 0\n","        for images, labels in test_progress_bar:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            predicts = (outputs > 0.5).float()\n","\n","            #Check for misclassifications\n","            for idx, (pred, true) in enumerate(zip(predicts, labels)):\n","                if not torch.equal(pred, true):\n","                    misclassified_images.append({\n","                        'image_index': image_index + idx,\n","                        'predicted_labels': pred.cpu().numpy(),\n","                        'true_labels': true.cpu().numpy()\n","                    })\n","\n","            # ... [rest of your existing code in the loop] ...\n","            # Store predictions and true labels for later metrics calculation\n","            all_preds.extend(predicts.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","            acc = (predicts == labels).sum().item() / (images.size(0) * num_classes)\n","            loss = criterion(outputs, labels)\n","\n","            test_loss = loss.item()\n","            test_running_loss += test_loss * images.size(0)\n","            test_correct_predictions += acc * images.size(0)\n","\n","            # Update the progress bar with the loss and accuracy\n","            test_progress_bar.set_postfix({'Batch Loss': test_loss, 'Batch Accuracy': acc})\n","\n","            image_index += images.size(0)\n","\n","    # Calculate average loss and accuracy for the test set\n","    test_epoch_loss = test_running_loss / len(test_loader.dataset)\n","    test_epoch_accuracy = test_correct_predictions / len(test_loader.dataset)\n","\n","    # Calculate other metrics\n","    precision = precision_score(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n","    recall = recall_score(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n","    f1 = f1_score(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n","    roc_auc = roc_auc_score(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n","    tn, fp, fn, tp = confusion_matrix(np.array(all_labels).flatten(), np.array(all_preds).flatten()).ravel()\n","\n","    print(\"\\n\" + \"=\"*50)\n","    print(f'Test Metrics:')\n","    print(f'{\"-\"*50}')\n","    print(f'Loss      : {test_epoch_loss:.4f}')\n","    print(f'Accuracy  : {test_epoch_accuracy:.4f}')\n","    print(f'Precision : {precision:.4f}')\n","    print(f'Recall    : {recall:.4f}')\n","    print(f'True Negatives : {tn}')\n","    print(f'False Positives: {fp}')\n","    print(f'False Negatives: {fn}')\n","    print(f'True Positives : {tp}')\n","    print(f'F1 Score  : {f1:.4f}')\n","    print(f'ROC AUC   : {roc_auc:.4f}')\n","    print(\"=\"*50)\n","\n","    # Initialize variables to store class-wise metrics\n","    class_precisions = []\n","    class_recalls = []\n","    class_f1s = []\n","    class_roc_aucs = []\n","\n","    # Calculate metrics for each class\n","    num_classes = np.array(all_labels).shape[1]  # Assuming all_labels is a 2D array\n","    for i in range(num_classes):\n","      y_true = np.array(all_labels)[:, i]\n","      y_pred = np.array(all_preds)[:, i]\n","\n","      precision = precision_score(y_true, y_pred, zero_division=0)\n","      recall = recall_score(y_true, y_pred, zero_division=0)\n","      f1 = f1_score(y_true, y_pred, zero_division=0)\n","\n","      # Only calculate ROC AUC if both classes (0 and 1) are present in y_true\n","      if len(np.unique(y_true)) > 1:\n","          roc_auc = roc_auc_score(y_true, y_pred)\n","      else:\n","          roc_auc = float('nan')  # or you can set this to 0, or any other default value\n","\n","      tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","\n","      class_precisions.append(precision)\n","      class_recalls.append(recall)\n","      class_f1s.append(f1)\n","      class_roc_aucs.append(roc_auc)\n","\n","      print(f\"Metrics for class {i}:\")\n","      print(f\"  Precision : {precision:.4f}\")\n","      print(f\"  Recall    : {recall:.4f}\")\n","      print(f\"  F1 Score  : {f1:.4f}\")\n","      print(f\"  ROC AUC   : {roc_auc:.4f}\")\n","      print(f'True Negatives : {tn}')\n","      print(f'False Positives: {fp}')\n","      print(f'False Negatives: {fn}')\n","      print(f'True Positives : {tp}')\n","      print(\"-\"*20)\n","\n","    # If you want a summary report\n","    print(\"Summary Classification Report:\")\n","    print(classification_report(np.array(all_labels), np.array(all_preds), zero_division=0))\n","\n","    # Print or return the misclassified images\n","    print(f\"Total Misclassified Images: {len(misclassified_images)}\")\n","    for misclassified in misclassified_images:\n","        print(f\"Image Index: {misclassified['image_index']}, Predicted Labels: {misclassified['predicted_labels']}, True Labels: {misclassified['true_labels']}\")\n","\n","    return all_labels, all_preds, misclassified_images"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"gwRSRZdnlmG7","executionInfo":{"status":"ok","timestamp":1731617218352,"user_tz":300,"elapsed":175,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"}}},"outputs":[],"source":["# Main function to run the whole pipeline\n","def main(root_dir, sub_folder, model, criterion, optimizer, num_epochs, data_type='original'):\n","    # Initialize Data\n","    train_loader, val_loader, test_loader = initialize_data(root_dir, sub_folder, data_type=data_type)\n","    #check_loaders(train_loader, val_loader, test_loader)\n","\n","    '''global device\n","    model = CustomResNet(num_classes).to(device)\n","    criterion = nn.BCELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)'''\n","\n","    # Train Model\n","    train_losses, train_accuracies, val_losses, val_accuracies = train_model(train_loader, val_loader, model, criterion, optimizer, num_epochs=num_epochs)\n","    plot_loss_acc(train_losses, train_accuracies, val_losses, val_accuracies)\n","\n","    # Test Model\n","    all_labels, all_preds = test_model(test_loader, model, criterion)\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"OMnkZMAso4k_","executionInfo":{"status":"ok","timestamp":1731617218352,"user_tz":300,"elapsed":1,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","def show_images(images, labels=None, num_images=4):\n","    fig, axs = plt.subplots(1, num_images, figsize=(15, 3))\n","    for i in range(num_images):\n","        axs[i].imshow(images[i].permute(1, 2, 0))  # Convert from (C, H, W) to (H, W, C)\n","        axs[i].axis('off')\n","        if labels is not None:\n","            axs[i].set_title(labels[i])\n","\n","def check_loaders(train_loader, val_loader, test_loader):\n","    print(\"Train Loader: {} batches ({} images)\".format(len(train_loader), len(train_loader.dataset)))\n","    print(\"Validation Loader: {} batches ({} images)\".format(len(val_loader), len(val_loader.dataset)))\n","    print(\"Test Loader: {} batches ({} images)\".format(len(test_loader), len(test_loader.dataset)))\n","\n","    # Optionally, visualize some images from each loader\n","    for images, labels in train_loader:\n","        show_images(images)\n","        break  # Just show the first batch"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"Cgv5wEuDnK51","executionInfo":{"status":"ok","timestamp":1731617218352,"user_tz":300,"elapsed":1,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"}}},"outputs":[],"source":["#train-valid-test\n","def tvt(train_loader, val_loader, test_loader, model, criterion, optimizer, num_epochs):\n","\n","    # Train Model\n","    train_losses, train_accuracies, val_losses, val_accuracies = train_model(train_loader, val_loader, model, criterion, optimizer, num_epochs=num_epochs)\n","    plot_loss_acc(train_losses, train_accuracies, val_losses, val_accuracies)\n","\n","    # Test Model\n","    all_labels, all_preds = test_model(test_loader, model, criterion)\n"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":62952,"status":"ok","timestamp":1731617281303,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"},"user_tz":300},"id":"kLWSjH6rna2Z","outputId":"b38eb9f0-c851-4f9a-fc70-8fbbb27c66e2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processing video folder: 01.09.31 hrs __0025097\n","Processing video folder: 01.09.47 hrs __0025098\n","Processing video folder: AM12 (Converted)\n","Processing video folder: BM12 (Converted)\n","Processing video folder: LP SONO 001 POST FNL CONUS\n","Processing video folder: LP SONO 001 POST FST CONUS\n","Processing video folder: LPPILOTLAT1\n","Processing video folder: LPPILOTLAT2\n","Processing video folder: LPPILOTLAT3\n","Processing video folder: LPPILOTSITTING2\n","Processing video folder: US00000L\n","Processing video folder: XM12 (Converted)\n","Processing video folder: YM12 (Converted)\n","Processing video folder: 200106061016030017SMP_crop\n","Processing video folder: 200106061016340018SMP_crop\n","Processing video folder: 200106061016530019SMP_crop\n","Processing video folder: 200106061017230020SMP_crop\n","Processing video folder: 202402091355160039ABD_crop\n","Processing video folder: 202402091355230040ABD_crop\n","Processing video folder: 202402091355350041ABD_crop\n","Processing video folder: 202402091355470042ABD_crop\n","Processing video folder: 202402091355560043ABD_crop\n","Processing video folder: 202402091356080044ABD_cropCONUS\n","Processing video folder: 202402091356160045ABD_crop\n","Processing video folder: 202402091356410046ABD_crop\n","Processing video folder: 202402091356500047ABD_crop\n","Processing video folder: 202402091356590048ABD_crop\n","Number of videos: 27\n","Data Size: train: 2849 val: 407 test: 815\n"]}],"source":["root_dir = '/content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files'    # CHANGE BASED ON FOLDER LOCATION\n","sub_folder = 'Old + New Video Frames'\n","num_classes = 3\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","org_train_loader, org_val_loader, org_test_loader = initialize_data(root_dir,\n","                                                                    sub_folder,\n","                                                                    data_type='original')"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"y-wwFYORpYEK","executionInfo":{"status":"ok","timestamp":1731617281303,"user_tz":300,"elapsed":2,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"}}},"outputs":[],"source":["from typing import Optional, Sequence\n","\n","import torch\n","from torch import Tensor\n","from torch import nn\n","from torch.nn import functional as F"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":615046,"status":"error","timestamp":1731617896348,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"},"user_tz":300},"id":"88Sbsz-wqHeq","outputId":"f73b274a-2b42-4e0d-ed86-5d61c4d8b54d"},"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|██████████| 90/90 [08:57<00:00,  5.98s/it, Batch Loss=1.01, Batch Accuracy=0.667]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/1 - Loss: 0.1789, Accuracy: 0.9318\n","Validation:\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|██████████| 13/13 [01:16<00:00,  5.85s/it, Batch Loss=0.186, Batch Accuracy=0.899]\n"]},{"output_type":"stream","name":"stdout","text":["Validation Epoch 1/1 - Loss: 0.1254, Accuracy: 0.9631\n","------------------------------------------------------------------------------------------------\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 1000x500 with 2 Axes>"],"image/png":"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\n"},"metadata":{}},{"output_type":"stream","name":"stderr","text":["<ipython-input-5-ae76e8f17af0>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model = torch.load(model_path)\n"]},{"output_type":"error","ename":"AttributeError","evalue":"'CustomResNet' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_check_seekable\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    756\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1930\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1931\u001b[0;31m         raise AttributeError(\n\u001b[0m\u001b[1;32m   1932\u001b[0m             \u001b[0;34mf\"'{type(self).__name__}' object has no attribute '{name}'\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'CustomResNet' object has no attribute 'seek'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-540a4ff496d1>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;31m# Run the main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# CHANGE EPOCH NUM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtvt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morg_train_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morg_val_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morg_test_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-9-50c1a8a95b77>\u001b[0m in \u001b[0;36mtvt\u001b[0;34m(train_loader, val_loader, test_loader, model, criterion, optimizer, num_epochs)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Test Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mall_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-6-9796b953bd61>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(test_loader, model_path, criterion)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m# Load the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mall_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-ae76e8f17af0>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_path)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# Load the saved model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set the model to evaluation mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1317\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1319\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m             \u001b[0;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_open_buffer_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_open_buffer_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected 'r' or 'w' in mode but got {mode}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, buffer)\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 649\u001b[0;31m         \u001b[0m_check_seekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    650\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    651\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_check_seekable\u001b[0;34m(f)\u001b[0m\n\u001b[1;32m    758\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnsupportedOperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 760\u001b[0;31m         \u001b[0mraise_err_msg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"seek\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"tell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    761\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mraise_err_msg\u001b[0;34m(patterns, e)\u001b[0m\n\u001b[1;32m    751\u001b[0m                     \u001b[0;34m+\u001b[0m \u001b[0;34m\" try to load from it instead.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m                 )\n\u001b[0;32m--> 753\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    754\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'CustomResNet' object has no attribute 'seek'. You can only torch.load from a file that is seekable. Please pre-load the data into a buffer like io.BytesIO and try to load from it instead."]}],"source":["if __name__ == \"__main__\":\n","    num_classes = 3\n","    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","    # Initialize Model, Loss, and Optimizer\n","    model = CustomResNet(num_classes).to(device)\n","    criterion = nn.BCELoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n","\n","    # Run the main function\n","    # CHANGE EPOCH NUM\n","    tvt(org_train_loader, org_val_loader, org_test_loader, model, criterion, optimizer, num_epochs=20)"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U0rj7ZJI9gpy","outputId":"3d0b7693-25a1-4e57-9539-69a68097784f","executionInfo":{"status":"ok","timestamp":1731618185024,"user_tz":300,"elapsed":153488,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-ae76e8f17af0>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model = torch.load(model_path)\n","Testing: 100%|██████████| 26/26 [02:33<00:00,  5.89s/it, Batch Loss=0.0627, Batch Accuracy=1]"]},{"output_type":"stream","name":"stdout","text":["\n","==================================================\n","Test Metrics:\n","--------------------------------------------------\n","Loss      : 0.0930\n","Accuracy  : 0.9742\n","Precision : 0.9667\n","Recall    : 0.9677\n","True Negatives : 1452\n","False Positives: 32\n","False Negatives: 31\n","True Positives : 930\n","F1 Score  : 0.9672\n","ROC AUC   : 0.9731\n","==================================================\n","Metrics for class 0:\n","  Precision : 0.9896\n","  Recall    : 0.8190\n","  F1 Score  : 0.8962\n","  ROC AUC   : 0.9088\n","True Negatives : 698\n","False Positives: 1\n","False Negatives: 21\n","True Positives : 95\n","--------------------\n","Metrics for class 1:\n","  Precision : 0.9486\n","  Recall    : 0.9575\n","  F1 Score  : 0.9531\n","  ROC AUC   : 0.9697\n","True Negatives : 592\n","False Positives: 11\n","False Negatives: 9\n","True Positives : 203\n","--------------------\n","Metrics for class 2:\n","  Precision : 0.9693\n","  Recall    : 0.9984\n","  F1 Score  : 0.9837\n","  ROC AUC   : 0.9443\n","True Negatives : 162\n","False Positives: 20\n","False Negatives: 1\n","True Positives : 632\n","--------------------\n","Summary Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.82      0.90       116\n","           1       0.95      0.96      0.95       212\n","           2       0.97      1.00      0.98       633\n","\n","   micro avg       0.97      0.97      0.97       961\n","   macro avg       0.97      0.92      0.94       961\n","weighted avg       0.97      0.97      0.97       961\n"," samples avg       0.91      0.90      0.90       961\n","\n","Total Misclassified Images: 41\n","Image Index: 2, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 55, Predicted Labels: [0. 0. 1.], True Labels: [0. 1. 1.]\n","Image Index: 62, Predicted Labels: [0. 0. 1.], True Labels: [0. 1. 1.]\n","Image Index: 81, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 100, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 112, Predicted Labels: [0. 0. 0.], True Labels: [1. 0. 0.]\n","Image Index: 135, Predicted Labels: [0. 1. 0.], True Labels: [1. 0. 0.]\n","Image Index: 141, Predicted Labels: [0. 0. 1.], True Labels: [0. 0. 0.]\n","Image Index: 159, Predicted Labels: [0. 1. 1.], True Labels: [0. 0. 1.]\n","Image Index: 185, Predicted Labels: [0. 0. 1.], True Labels: [0. 1. 1.]\n","Image Index: 186, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 211, Predicted Labels: [0. 0. 1.], True Labels: [0. 0. 0.]\n","Image Index: 247, Predicted Labels: [0. 0. 1.], True Labels: [0. 0. 0.]\n","Image Index: 248, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 289, Predicted Labels: [0. 0. 1.], True Labels: [0. 1. 1.]\n","Image Index: 309, Predicted Labels: [0. 0. 1.], True Labels: [0. 1. 1.]\n","Image Index: 344, Predicted Labels: [0. 0. 0.], True Labels: [0. 0. 1.]\n","Image Index: 348, Predicted Labels: [0. 0. 1.], True Labels: [0. 1. 1.]\n","Image Index: 374, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 386, Predicted Labels: [0. 0. 1.], True Labels: [0. 0. 0.]\n","Image Index: 395, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 397, Predicted Labels: [0. 1. 0.], True Labels: [1. 0. 0.]\n","Image Index: 402, Predicted Labels: [0. 0. 0.], True Labels: [1. 0. 0.]\n","Image Index: 457, Predicted Labels: [0. 1. 0.], True Labels: [1. 0. 0.]\n","Image Index: 478, Predicted Labels: [0. 1. 1.], True Labels: [0. 0. 1.]\n","Image Index: 483, Predicted Labels: [0. 0. 1.], True Labels: [0. 0. 0.]\n","Image Index: 496, Predicted Labels: [0. 1. 1.], True Labels: [0. 0. 1.]\n","Image Index: 529, Predicted Labels: [0. 1. 1.], True Labels: [0. 0. 1.]\n","Image Index: 563, Predicted Labels: [0. 0. 1.], True Labels: [0. 1. 1.]\n","Image Index: 573, Predicted Labels: [0. 0. 1.], True Labels: [0. 0. 0.]\n","Image Index: 580, Predicted Labels: [0. 0. 0.], True Labels: [1. 0. 0.]\n","Image Index: 583, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 616, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 621, Predicted Labels: [0. 1. 1.], True Labels: [1. 0. 0.]\n","Image Index: 655, Predicted Labels: [0. 1. 1.], True Labels: [1. 0. 0.]\n","Image Index: 666, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 678, Predicted Labels: [0. 1. 0.], True Labels: [1. 0. 0.]\n","Image Index: 717, Predicted Labels: [0. 1. 1.], True Labels: [1. 0. 0.]\n","Image Index: 744, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 749, Predicted Labels: [0. 0. 1.], True Labels: [0. 1. 1.]\n","Image Index: 796, Predicted Labels: [1. 0. 0.], True Labels: [0. 1. 0.]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}],"source":["criterion = nn.BCELoss()\n","all_labels, all_preds, misclassified_images = test_model(org_test_loader, \"saved_model.pt\", criterion)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"RC2U8ofo-f5p","executionInfo":{"status":"ok","timestamp":1731618217988,"user_tz":300,"elapsed":160,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"}}},"outputs":[],"source":["# Test Model\n","def test_miss_model(test_loader, model_path, criterion):\n","    global num_classes\n","    global device\n","    # Initialize variables to store predictions and true labels\n","\n","    # Load the model\n","    model = load_model(model_path)\n","\n","    all_preds = []\n","    all_labels = []\n","\n","    # Test loop\n","    model.eval()  # Set the model to evaluation mode\n","    test_running_loss = 0.0\n","    test_correct_predictions = 0\n","\n","    # Initialize the progress bar for testing\n","    test_progress_bar = tqdm(test_loader, total=len(test_loader), desc='Testing', position=0, leave=True)\n","\n","    misclassified_images = []\n","\n","    with torch.no_grad():\n","        image_index = 0\n","        for images, labels in test_progress_bar:\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            predicts = (outputs > 0.5).float()\n","\n","            # Check for misclassifications\n","            for idx, (img, pred, true) in enumerate(zip(images, predicts, labels)):\n","                if not torch.equal(pred, true):\n","                    misclassified_images.append({\n","                        'image_index': image_index + idx,  # Store the index\n","                        'image': img.cpu(),  # Store the image tensor\n","                        'predicted_labels': pred.cpu().numpy(),\n","                        'true_labels': true.cpu().numpy()\n","                    })\n","\n","            # ... [rest of your existing code in the loop] ...\n","            # Store predictions and true labels for later metrics calculation\n","            all_preds.extend(predicts.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","            acc = (predicts == labels).sum().item() / (images.size(0) * num_classes)\n","            loss = criterion(outputs, labels)\n","\n","            test_loss = loss.item()\n","            test_running_loss += test_loss * images.size(0)\n","            test_correct_predictions += acc * images.size(0)\n","\n","            # Update the progress bar with the loss and accuracy\n","            test_progress_bar.set_postfix({'Batch Loss': test_loss, 'Batch Accuracy': acc})\n","\n","            image_index += images.size(0)\n","\n","    # Calculate average loss and accuracy for the test set\n","    test_epoch_loss = test_running_loss / len(test_loader.dataset)\n","    test_epoch_accuracy = test_correct_predictions / len(test_loader.dataset)\n","\n","    # Calculate other metrics\n","    precision = precision_score(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n","    recall = recall_score(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n","    f1 = f1_score(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n","    roc_auc = roc_auc_score(np.array(all_labels).flatten(), np.array(all_preds).flatten())\n","    tn, fp, fn, tp = confusion_matrix(np.array(all_labels).flatten(), np.array(all_preds).flatten()).ravel()\n","\n","    print(\"\\n\" + \"=\"*50)\n","    print(f'Test Metrics:')\n","    print(f'{\"-\"*50}')\n","    print(f'Loss      : {test_epoch_loss:.4f}')\n","    print(f'Accuracy  : {test_epoch_accuracy:.4f}')\n","    print(f'Precision : {precision:.4f}')\n","    print(f'Recall    : {recall:.4f}')\n","    print(f'True Negatives : {tn}')\n","    print(f'False Positives: {fp}')\n","    print(f'False Negatives: {fn}')\n","    print(f'True Positives : {tp}')\n","    print(f'F1 Score  : {f1:.4f}')\n","    print(f'ROC AUC   : {roc_auc:.4f}')\n","    print(\"=\"*50)\n","\n","    # Initialize variables to store class-wise metrics\n","    class_precisions = []\n","    class_recalls = []\n","    class_f1s = []\n","    class_roc_aucs = []\n","\n","    # Calculate metrics for each class\n","    num_classes = np.array(all_labels).shape[1]  # Assuming all_labels is a 2D array\n","    for i in range(num_classes):\n","        y_true = np.array(all_labels)[:, i]\n","        y_pred = np.array(all_preds)[:, i]\n","\n","        precision = precision_score(y_true, y_pred, zero_division=0)\n","        recall = recall_score(y_true, y_pred, zero_division=0)\n","        f1 = f1_score(y_true, y_pred, zero_division=0)\n","        roc_auc = roc_auc_score(y_true, y_pred)\n","        tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n","\n","        class_precisions.append(precision)\n","        class_recalls.append(recall)\n","        class_f1s.append(f1)\n","        class_roc_aucs.append(roc_auc)\n","\n","        print(f\"Metrics for class {i}:\")\n","        print(f\"  Precision : {precision:.4f}\")\n","        print(f\"  Recall    : {recall:.4f}\")\n","        print(f\"  F1 Score  : {f1:.4f}\")\n","        print(f\"  ROC AUC   : {roc_auc:.4f}\")\n","        print(f'True Negatives : {tn}')\n","        print(f'False Positives: {fp}')\n","        print(f'False Negatives: {fn}')\n","        print(f'True Positives : {tp}')\n","        print(\"-\"*20)\n","\n","    # If you want a summary report\n","    print(\"Summary Classification Report:\")\n","    print(classification_report(np.array(all_labels), np.array(all_preds), zero_division=0))\n","\n","    # Print or return the misclassified images\n","    print(f\"Total Misclassified Images: {len(misclassified_images)}\")\n","    for misclassified in misclassified_images:\n","        print(f\"Image Index: {misclassified['image_index']}, Predicted Labels: {misclassified['predicted_labels']}, True Labels: {misclassified['true_labels']}\")\n","\n","    return all_labels, all_preds, misclassified_images"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":7938,"status":"ok","timestamp":1731618230303,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"},"user_tz":300},"id":"iWoh5VADDawk","colab":{"base_uri":"https://localhost:8080/"},"outputId":"2e809609-3250-4473-a2fd-5833ad5d3892"},"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-5-ae76e8f17af0>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model = torch.load(model_path)\n","Testing: 100%|██████████| 26/26 [00:07<00:00,  3.39it/s, Batch Loss=0.0627, Batch Accuracy=1]\n"]},{"output_type":"stream","name":"stdout","text":["\n","==================================================\n","Test Metrics:\n","--------------------------------------------------\n","Loss      : 0.0930\n","Accuracy  : 0.9742\n","Precision : 0.9667\n","Recall    : 0.9677\n","True Negatives : 1452\n","False Positives: 32\n","False Negatives: 31\n","True Positives : 930\n","F1 Score  : 0.9672\n","ROC AUC   : 0.9731\n","==================================================\n","Metrics for class 0:\n","  Precision : 0.9896\n","  Recall    : 0.8190\n","  F1 Score  : 0.8962\n","  ROC AUC   : 0.9088\n","True Negatives : 698\n","False Positives: 1\n","False Negatives: 21\n","True Positives : 95\n","--------------------\n","Metrics for class 1:\n","  Precision : 0.9486\n","  Recall    : 0.9575\n","  F1 Score  : 0.9531\n","  ROC AUC   : 0.9697\n","True Negatives : 592\n","False Positives: 11\n","False Negatives: 9\n","True Positives : 203\n","--------------------\n","Metrics for class 2:\n","  Precision : 0.9693\n","  Recall    : 0.9984\n","  F1 Score  : 0.9837\n","  ROC AUC   : 0.9443\n","True Negatives : 162\n","False Positives: 20\n","False Negatives: 1\n","True Positives : 632\n","--------------------\n","Summary Classification Report:\n","              precision    recall  f1-score   support\n","\n","           0       0.99      0.82      0.90       116\n","           1       0.95      0.96      0.95       212\n","           2       0.97      1.00      0.98       633\n","\n","   micro avg       0.97      0.97      0.97       961\n","   macro avg       0.97      0.92      0.94       961\n","weighted avg       0.97      0.97      0.97       961\n"," samples avg       0.91      0.90      0.90       961\n","\n","Total Misclassified Images: 41\n","Image Index: 2, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 55, Predicted Labels: [0. 0. 1.], True Labels: [0. 1. 1.]\n","Image Index: 62, Predicted Labels: [0. 0. 1.], True Labels: [0. 1. 1.]\n","Image Index: 81, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 100, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 112, Predicted Labels: [0. 0. 0.], True Labels: [1. 0. 0.]\n","Image Index: 135, Predicted Labels: [0. 1. 0.], True Labels: [1. 0. 0.]\n","Image Index: 141, Predicted Labels: [0. 0. 1.], True Labels: [0. 0. 0.]\n","Image Index: 159, Predicted Labels: [0. 1. 1.], True Labels: [0. 0. 1.]\n","Image Index: 185, Predicted Labels: [0. 0. 1.], True Labels: [0. 1. 1.]\n","Image Index: 186, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 211, Predicted Labels: [0. 0. 1.], True Labels: [0. 0. 0.]\n","Image Index: 247, Predicted Labels: [0. 0. 1.], True Labels: [0. 0. 0.]\n","Image Index: 248, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 289, Predicted Labels: [0. 0. 1.], True Labels: [0. 1. 1.]\n","Image Index: 309, Predicted Labels: [0. 0. 1.], True Labels: [0. 1. 1.]\n","Image Index: 344, Predicted Labels: [0. 0. 0.], True Labels: [0. 0. 1.]\n","Image Index: 348, Predicted Labels: [0. 0. 1.], True Labels: [0. 1. 1.]\n","Image Index: 374, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 386, Predicted Labels: [0. 0. 1.], True Labels: [0. 0. 0.]\n","Image Index: 395, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 397, Predicted Labels: [0. 1. 0.], True Labels: [1. 0. 0.]\n","Image Index: 402, Predicted Labels: [0. 0. 0.], True Labels: [1. 0. 0.]\n","Image Index: 457, Predicted Labels: [0. 1. 0.], True Labels: [1. 0. 0.]\n","Image Index: 478, Predicted Labels: [0. 1. 1.], True Labels: [0. 0. 1.]\n","Image Index: 483, Predicted Labels: [0. 0. 1.], True Labels: [0. 0. 0.]\n","Image Index: 496, Predicted Labels: [0. 1. 1.], True Labels: [0. 0. 1.]\n","Image Index: 529, Predicted Labels: [0. 1. 1.], True Labels: [0. 0. 1.]\n","Image Index: 563, Predicted Labels: [0. 0. 1.], True Labels: [0. 1. 1.]\n","Image Index: 573, Predicted Labels: [0. 0. 1.], True Labels: [0. 0. 0.]\n","Image Index: 580, Predicted Labels: [0. 0. 0.], True Labels: [1. 0. 0.]\n","Image Index: 583, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 616, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 621, Predicted Labels: [0. 1. 1.], True Labels: [1. 0. 0.]\n","Image Index: 655, Predicted Labels: [0. 1. 1.], True Labels: [1. 0. 0.]\n","Image Index: 666, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 678, Predicted Labels: [0. 1. 0.], True Labels: [1. 0. 0.]\n","Image Index: 717, Predicted Labels: [0. 1. 1.], True Labels: [1. 0. 0.]\n","Image Index: 744, Predicted Labels: [0. 0. 1.], True Labels: [1. 0. 0.]\n","Image Index: 749, Predicted Labels: [0. 0. 1.], True Labels: [0. 1. 1.]\n","Image Index: 796, Predicted Labels: [1. 0. 0.], True Labels: [0. 1. 0.]\n"]}],"source":["criterion = nn.BCELoss()\n","all_labels, all_preds, misclassified_images = test_miss_model(org_test_loader, \"saved_model.pt\", criterion)"]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","from PIL import Image\n","import os\n","from tqdm import tqdm\n","\n","# Define device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Define Dataset for Inference\n","class InferenceDataset(Dataset):\n","    def __init__(self, root_dir, transform):\n","        self.root_dir = root_dir\n","        self.transform = transform\n","        # Collecting image paths with a debug message\n","        self.image_paths = [os.path.join(root_dir, img) for img in os.listdir(root_dir) if img.endswith(\".JPG\")]\n","        if not self.image_paths:\n","            print(f\"No images found in {root_dir}. Please check the directory path and file extensions.\")\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.image_paths[idx]\n","        image = Image.open(img_path).convert('RGB')\n","        if self.transform:\n","            image = self.transform(image)\n","        return image, img_path  # Return path as well for identification\n","\n","# Function to Generate Predictions\n","def generate_predictions(model_path, inference_folder):\n","    # Load the saved model and move it to the device\n","    model = torch.load(model_path, map_location=device)\n","    model.eval()  # Set the model to evaluation mode\n","    model.to(device)\n","\n","    # Define image transformations\n","    transform = transforms.Compose([transforms.Resize((224, 224)), transforms.ToTensor()])\n","\n","    # Initialize dataset and dataloader\n","    inference_dataset = InferenceDataset(root_dir=inference_folder, transform=transform)\n","    inference_loader = DataLoader(inference_dataset, batch_size=32, shuffle=False)\n","\n","    predictions = []\n","    with torch.no_grad():\n","        for images, img_paths in tqdm(inference_loader, desc='Generating Predictions'):\n","            images = images.to(device)\n","            outputs = model(images)\n","\n","            # Convert predictions to binary values\n","            preds = (outputs > 0.5).float().cpu().numpy()\n","\n","            # Collect predictions and corresponding image paths\n","            for img_path, pred in zip(img_paths, preds):\n","                predictions.append({\n","                    \"image_path\": img_path,\n","                    \"predicted_labels\": pred\n","                })\n","\n","    if not predictions:\n","        print(\"No predictions generated. Please check if the dataset is loading images correctly.\")\n","    return predictions\n","\n","# Folder containing new images for inference\n","inference_folder = \"/content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict\"\n","model_path = \"saved_model.pt\"  # Path to your saved model\n","\n","# Generate predictions\n","predictions = generate_predictions(model_path, inference_folder)\n","\n","# Display predictions\n","for pred in predictions:\n","    print(f\"Image: {pred['image_path']}, Predicted Labels: {pred['predicted_labels']}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jO1BZ0eLbYIx","executionInfo":{"status":"ok","timestamp":1731618766663,"user_tz":300,"elapsed":13006,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"}},"outputId":"0744a5db-da54-4a24-ecd7-f8c20d3b13cd"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["<ipython-input-25-9798fb339ce2>:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  model = torch.load(model_path, map_location=device)\n","Generating Predictions: 100%|██████████| 1/1 [00:12<00:00, 12.79s/it]"]},{"output_type":"stream","name":"stdout","text":["Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0267.JPG, Predicted Labels: [0. 1. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0271.JPG, Predicted Labels: [0. 0. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0289.JPG, Predicted Labels: [0. 0. 0.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0254.JPG, Predicted Labels: [0. 1. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0277.JPG, Predicted Labels: [0. 1. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0268.JPG, Predicted Labels: [0. 1. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0266.JPG, Predicted Labels: [0. 0. 0.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0307.JPG, Predicted Labels: [0. 0. 0.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0303.JPG, Predicted Labels: [0. 0. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0305.JPG, Predicted Labels: [0. 1. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0310.JPG, Predicted Labels: [0. 1. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0301.JPG, Predicted Labels: [0. 1. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0312.JPG, Predicted Labels: [0. 1. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0291.JPG, Predicted Labels: [0. 0. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0293.JPG, Predicted Labels: [0. 0. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0290.JPG, Predicted Labels: [0. 0. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0255.JPG, Predicted Labels: [0. 0. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0319.JPG, Predicted Labels: [0. 0. 0.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0316.JPG, Predicted Labels: [0. 0. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0341.JPG, Predicted Labels: [0. 1. 0.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0317.JPG, Predicted Labels: [0. 1. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0340.JPG, Predicted Labels: [0. 1. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0337.JPG, Predicted Labels: [0. 1. 0.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0339.JPG, Predicted Labels: [1. 0. 0.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0347.JPG, Predicted Labels: [0. 0. 0.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0354.JPG, Predicted Labels: [0. 1. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0356.JPG, Predicted Labels: [0. 0. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/MG_0362.JPG, Predicted Labels: [0. 1. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0364.JPG, Predicted Labels: [0. 1. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0365.JPG, Predicted Labels: [0. 1. 1.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0366.JPG, Predicted Labels: [0. 0. 0.]\n","Image: /content/drive/MyDrive/Fall 2024 Classes/Capstone: CV for Infant Lumbar Puncture/Data Files/Images to Predict/IMG_0315.JPG, Predicted Labels: [0. 0. 0.]\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yyTGf3ZxFjnK","executionInfo":{"status":"aborted","timestamp":1731617896348,"user_tz":300,"elapsed":4,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"}}},"outputs":[],"source":["#function for all missclassified images\n","\n","import matplotlib.pyplot as plt\n","import math\n","\n","def show_misclassified_images(misclassified_images):\n","    num_images = len(misclassified_images)\n","    num_cols = 5  # You can adjust the number of columns\n","    num_rows = math.ceil(num_images / num_cols)\n","\n","    fig, axs = plt.subplots(num_rows, num_cols, figsize=(20, 4 * num_rows))\n","\n","    for i in range(num_images):\n","        img_info = misclassified_images[i]\n","        img = img_info['image'].permute(1, 2, 0)  # Convert from CxHxW to HxWxC\n","        predicted = img_info['predicted_labels']\n","        true = img_info['true_labels']\n","        index = img_info['image_index']  # Get the image index\n","\n","        ax = axs[i // num_cols, i % num_cols]  # Determine row and column position\n","        ax.imshow(img)\n","        ax.set_title(f\"Index: {index}\\nPred: {predicted}\\nTrue: {true}\")\n","        ax.axis('off')\n","\n","    # Hide any unused subplots\n","    for i in range(num_images, num_rows * num_cols):\n","        axs[i // num_cols, i % num_cols].axis('off')\n","\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1731617896348,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"},"user_tz":300},"id":"n2oc8WjRE8Na"},"outputs":[],"source":["# Now display the misclassified images\n","show_misclassified_images(misclassified_images)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"aborted","timestamp":1731617896348,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"},"user_tz":300},"id":"jgImz_nfFpLy"},"outputs":[],"source":["#calling all misclassified images\n","\n","show_misclassified_images(misclassified_images)"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1731617896348,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"},"user_tz":300},"id":"SCSPJVRwGCl8"},"outputs":[],"source":["def calculate_fp_fn(misclassified_images, num_classes):\n","    # Initialize counters\n","    false_positives = [0] * num_classes\n","    false_negatives = [0] * num_classes\n","\n","    # Analyze each misclassified image\n","    for img_info in misclassified_images:\n","        predicted_labels = img_info['predicted_labels']\n","        true_labels = img_info['true_labels']\n","\n","        for i in range(num_classes):\n","            # False positive: predicted is 1, true is 0\n","            if predicted_labels[i] == 1 and true_labels[i] == 0:\n","                false_positives[i] += 1\n","            # False negative: predicted is 0, true is 1\n","            elif predicted_labels[i] == 0 and true_labels[i] == 1:\n","                false_negatives[i] += 1\n","\n","    return false_positives, false_negatives\n","\n","# Call this function after you have the misclassified_images list\n","false_positives, false_negatives = calculate_fp_fn(misclassified_images, 3)\n","\n","# Print the results\n","print(\"False Positives per Class:\", false_positives)\n","print(\"False Negatives per Class:\", false_negatives)\n"]},{"cell_type":"markdown","metadata":{"id":"22dXPi2dIUqM"},"source":["False Positives per Class: [11, 0, 0]\n","\n","For the first class (let's call it Class 1), your model incorrectly predicted the positive label (predicted as '1') 11 times when the true label was negative (actual '0'). This means there were 11 instances where the model thought the feature corresponding to Class 1 was present, but it actually wasn't.\n","For the second and third classes (Class 2 and Class 3), there were no false positives. This means the model never incorrectly predicted the presence of these features; it did not mistakenly identify these features in any of the images where they were actually absent.\n","False Negatives per Class: [0, 3, 10]\n","\n","For Class 1, there were no false negatives. This means that whenever the feature corresponding to Class 1 was present in an image, the model always identified it correctly.\n","For Class 2, there were 3 instances where the model failed to identify the presence of the feature (predicted as '0') when it was actually present (true '1'). In other words, the model missed this feature 3 times when it should have detected it.\n","For Class 3, the situation is more pronounced with 10 false negatives. This indicates that the model frequently failed to recognize the presence of the feature corresponding to Class 3."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"e7WpWtnOGzhX","executionInfo":{"status":"aborted","timestamp":1731617896348,"user_tz":300,"elapsed":3,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"}}},"outputs":[],"source":["def filter_specific_misclassifications(misclassified_images):\n","    filtered_images = []\n","    for img_info in misclassified_images:\n","        true_labels = img_info['true_labels']\n","        predicted_labels = img_info['predicted_labels']\n","        # Check if Class 1 is '1' and Class 2 & 3 are '0' in true labels\n","        # and if Class 1 is correctly predicted as '1'\n","        if true_labels[0] == 1 and true_labels[1] == 0 and true_labels[2] == 0 and predicted_labels[0] == 1:\n","            filtered_images.append(img_info)\n","    return filtered_images\n","\n","specific_misclassifications = filter_specific_misclassifications(misclassified_images)\n","\n","def show_filtered_images(filtered_images, num_cols=5):\n","    num_images = len(filtered_images)\n","    if num_images == 0:\n","        print(\"No images meet the specified criteria.\")\n","        return\n","\n","    num_rows = math.ceil(num_images / num_cols)\n","\n","    fig, axs = plt.subplots(num_rows, num_cols, figsize=(20, 4 * num_rows))\n","\n","    for i, img_info in enumerate(filtered_images):\n","        img = img_info['image'].permute(1, 2, 0)  # Convert from CxHxW to HxWxC\n","        predicted = img_info['predicted_labels']\n","        true = img_info['true_labels']\n","\n","        # Handle the case when axs is 1-dimensional\n","        if num_images <= num_cols:\n","            ax = axs[i]  # Index as a 1D array\n","        else:\n","            ax = axs[i // num_cols, i % num_cols]  # Index as a 2D array\n","\n","        ax.imshow(img)\n","        ax.set_title(f\"Pred: {predicted}\\nTrue: {true}\")\n","        ax.axis('off')\n","\n","    # Hide any unused subplots\n","    for j in range(i + 1, num_rows * num_cols):\n","        if num_images <= num_cols:\n","            axs[j].axis('off')  # Index as a 1D array\n","        else:\n","            axs[j // num_cols, j % num_cols].axis('off')  # Index as a 2D array\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1731617896348,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"},"user_tz":300},"id":"BFMl7Q2cLGCI"},"outputs":[],"source":["# Filter and show the specific misclassifications\n","show_filtered_images(specific_misclassifications)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TafL0ROiLLvd","executionInfo":{"status":"aborted","timestamp":1731617896348,"user_tz":300,"elapsed":3,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"}}},"outputs":[],"source":["def filter_specific_predictions(misclassified_images):\n","    filtered_images = []\n","    for img_info in misclassified_images:\n","        predicted_labels = img_info['predicted_labels']\n","        # Check if predicted Class 1 is '1' and Classes 2 & 3 are '0'\n","        if predicted_labels[0] == 1 and predicted_labels[1] == 0 and predicted_labels[2] == 0:\n","            filtered_images.append(img_info)\n","    return filtered_images\n","\n","specific_predictions = filter_specific_predictions(misclassified_images)\n","\n","import matplotlib.pyplot as plt\n","import math\n","\n","def show_filtered_images(filtered_images, num_cols=5):\n","    num_images = len(filtered_images)\n","    if num_images == 0:\n","        print(\"No images meet the specified criteria.\")\n","        return\n","\n","    num_rows = math.ceil(num_images / num_cols)\n","    fig, axs = plt.subplots(num_rows, num_cols, figsize=(20, 4 * num_rows))\n","    # If there's only one row, axs will be 1D, so we need to handle this case\n","    if num_rows == 1:\n","        axs = [axs]  # Make axs a list of lists\n","\n","    for i, img_info in enumerate(filtered_images):\n","        img = img_info['image'].permute(1, 2, 0)  # Convert from CxHxW to HxWxC\n","        predicted = img_info['predicted_labels']\n","        true = img_info['true_labels']\n","        ax = axs[i // num_cols][i % num_cols] # Access the correct element now that axs is 2D\n","        ax.imshow(img)\n","        ax.set_title(f\"Pred: {predicted}\\nTrue: {true}\")\n","        ax.axis('off')\n","\n","    # Hide any unused subplots\n","    for j in range(i + 1, num_rows * num_cols):\n","        axs[j // num_cols][j % num_cols].axis('off')  # Access correctly\n","\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":3,"status":"aborted","timestamp":1731617896348,"user":{"displayName":"Isaac Tucker Peabody","userId":"13526994232413577315"},"user_tz":300},"id":"5_CkQFfMMcCh"},"outputs":[],"source":["# Filter and show the specific predictions\n","show_filtered_images(specific_predictions)"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[{"file_id":"1uKzPiAb0dbGPF5JJ19lyiSXZbuLghwq5","timestamp":1727644489319}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}